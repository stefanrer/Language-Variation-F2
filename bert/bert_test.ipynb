{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Not Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1844657841f3880b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:28.727811Z",
     "start_time": "2024-06-03T00:16:28.698804Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast, AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robst\\anaconda3\\envs\\RCCKI_W2V_Parser\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ai-forever/ruBert-large were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"ai-forever/ruBert-large\")\n",
    "model.load_state_dict(torch.load('model/bert_best_model.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:32.690984Z",
     "start_time": "2024-06-03T00:16:28.730731Z"
    }
   },
   "id": "ec5466d5560fd2e0",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-large\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:33.492558Z",
     "start_time": "2024-06-03T00:16:32.691957Z"
    }
   },
   "id": "846481cc9d40f89d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:33.713823Z",
     "start_time": "2024-06-03T00:16:33.492558Z"
    }
   },
   "id": "b944db6152f9a9c",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "special_tokens = [\"[CLS]\", \"[MASK]\", \"[PAD]\", \"[SEP]\", \"[UNK]\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:33.720672Z",
     "start_time": "2024-06-03T00:16:33.715801Z"
    }
   },
   "id": "6f93142fc5bad558",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_tokens(sentence, amount = 30):\n",
    "    # Encode the sentence and find the position of the [MASK] token\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=64, truncation=True)\n",
    "    mask_pos = torch.where(inputs.input_ids[0] == tokenizer.mask_token_id)[0].tolist()\n",
    "    # Move tensors to the same device as the model\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    # Get the model's predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits\n",
    "    # Get the top k predicted token IDs and convert them to tokens\n",
    "    top_k_predicted_token_ids = torch.topk(predictions[0, mask_pos, :], amount).indices.tolist()\n",
    "    \n",
    "    print(torch.topk(predictions[0, mask_pos, :], amount).indices.tolist())\n",
    "    \n",
    "    # Flatten the list of lists\n",
    "    flat_predicted_token_ids = [item for sublist in top_k_predicted_token_ids for item in sublist]\n",
    "    \n",
    "    # Convert the token IDs to tokens\n",
    "    top_k_predicted_tokens = tokenizer.convert_ids_to_tokens(flat_predicted_token_ids)\n",
    "    \n",
    "    predicted_tokens = [token for token in top_k_predicted_tokens if token not in special_tokens]\n",
    "    \n",
    "    return predicted_tokens\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:33.731004Z",
     "start_time": "2024-06-03T00:16:33.721645Z"
    }
   },
   "id": "4e5a7bbe6ca9aeee",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27219, 23284, 23708, 48313, 107515, 85191, 28608, 100, 17612, 90407, 75877, 14766, 44807, 15335, 28170, 31302, 7429, 44813, 100589, 56874, 30134, 49190, 39558, 33710, 35149, 62997, 49278, 4252, 49667, 32857]]\n",
      "[[27219, 100, 7429, 10136, 16775, 75199, 90407, 44813, 15335, 3034, 42588, 19890, 9351, 36694, 32600, 103634, 30983, 14766, 61537, 13035, 119293, 14627, 1019, 14114, 72199, 75877, 42833, 23708, 17934, 50355]]\n",
      "Predictions for sentence 1: следствие просило суд об [MASK]\n",
      "{'отставке', 'увеличении', 'ответственности', 'аресте', 'изменении', 'изоляции', 'отсрочке', 'экстрадиции', 'удалении', 'усилении', 'исключении', 'отводе', 'освобождении', 'избрании', 'увольнении', 'отстранении', 'ужесточении', 'условиях', 'отмене', 'утверждении', 'исполнении', 'ограничении', 'остановке', 'обеспечении', 'экспертизе', 'уничтожении', 'амнистии', 'оценке', 'удовлетворении'}\n",
      "Predictions for sentence 2: следствие ходатайствовало об [MASK] перед судом\n",
      "{'обстановке', 'организации', 'ответственности', 'аресте', 'источнике', 'обоснованности', 'измене', 'инициативе', 'открытии', 'инциденте', 'исключении', 'избрании', 'убийстве', 'избиении', 'угрозе', 'обстоятельствах', 'обмане', 'ошибке', 'ответе', 'отмене', 'убийце', 'отсутствии', 'исполнении', 'участии', 'обнаружении', 'аварии', 'этом', 'экспертизе', 'оценке'}\n",
      "Predictions intersect:\n",
      " {'исполнении', 'ответственности', 'аресте', 'исключении', 'экспертизе', 'отмене', 'оценке', 'избрании'}\n",
      "следствие просило суд об [исполнении] --- следствие ходатайствовало об [исполнении] перед судом\n",
      "следствие просило суд об [ответственности] --- следствие ходатайствовало об [ответственности] перед судом\n",
      "следствие просило суд об [аресте] --- следствие ходатайствовало об [аресте] перед судом\n",
      "следствие просило суд об [исключении] --- следствие ходатайствовало об [исключении] перед судом\n",
      "следствие просило суд об [экспертизе] --- следствие ходатайствовало об [экспертизе] перед судом\n",
      "следствие просило суд об [отмене] --- следствие ходатайствовало об [отмене] перед судом\n",
      "следствие просило суд об [оценке] --- следствие ходатайствовало об [оценке] перед судом\n",
      "следствие просило суд об [избрании] --- следствие ходатайствовало об [избрании] перед судом\n"
     ]
    }
   ],
   "source": [
    "# Example sentence with a masked token\n",
    "sentence1 = \"следствие просило суд об [MASK]\"\n",
    "sentence2 = \"следствие ходатайствовало об [MASK] перед судом\"\n",
    "\n",
    "x = set(predict_tokens(sentence1))\n",
    "y = set(predict_tokens(sentence2))\n",
    "\n",
    "predicted_intersect = x.intersection(y)\n",
    "print(f\"Predictions for sentence 1: {sentence1}\\n{x}\")\n",
    "print(f\"Predictions for sentence 2: {sentence2}\\n{y}\")\n",
    "print(f\"Predictions intersect:\\n {predicted_intersect}\")\n",
    "\n",
    "for predicted_token in predicted_intersect:\n",
    "    print(f\"{sentence1.replace('MASK', predicted_token)} --- {sentence2.replace('MASK', predicted_token)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:33.798914Z",
     "start_time": "2024-06-03T00:16:33.731977Z"
    }
   },
   "id": "c9fc1175c408209a",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f213efc1e14ea4a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the pipeline with top_k set to 30\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:34.053648Z",
     "start_time": "2024-06-03T00:16:33.799888Z"
    }
   },
   "id": "e12dfe76617ef534",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_token_dict(result):\n",
    "    token_dict = {}\n",
    "    for token in result:\n",
    "        if token[\"token_str\"] not in special_tokens:\n",
    "            token_dict[token[\"token\"]] = {\n",
    "                \"token_str\": token[\"token_str\"],\n",
    "                \"token_score\": token[\"score\"],\n",
    "                \"token_sequence\": token[\"sequence\"]\n",
    "            }\n",
    "        else:\n",
    "            continue\n",
    "    return token_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:34.059535Z",
     "start_time": "2024-06-03T00:16:34.054664Z"
    }
   },
   "id": "f75eaea49d6da3bd",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "следствие просило суд об ответственности(ответственности:0.0001128696821979247) --- следствие ходатаиствовало об ответственности перед судом(ответственности:0.0408891998231411)\n",
      "следствие просило суд об исключении(исключении:0.00023277508444152772) --- следствие ходатаиствовало об исключении перед судом(исключении:0.0019046157831326127)\n",
      "следствие просило суд об экспертизе(экспертизе:0.0004230121267028153) --- следствие ходатаиствовало об экспертизе перед судом(экспертизе:0.008529109880328178)\n",
      "следствие просило суд об исполнении(исполнении:0.0001663137081777677) --- следствие ходатаиствовало об исполнении перед судом(исполнении:0.007433668244630098)\n",
      "следствие просило суд об избрании(избрании:9.460232831770554e-05) --- следствие ходатаиствовало об избрании перед судом(избрании:0.007534338627010584)\n",
      "следствие просило суд об оценке(оценке:0.00019313418306410313) --- следствие ходатаиствовало об оценке перед судом(оценке:0.0037572649307549)\n",
      "следствие просило суд об аресте(аресте:0.9855115413665771) --- следствие ходатаиствовало об аресте перед судом(аресте:0.6155233383178711)\n",
      "следствие просило суд об отмене(отмене:0.0013841703766956925) --- следствие ходатаиствовало об отмене перед судом(отмене:0.0017304520588368177)\n"
     ]
    }
   ],
   "source": [
    "sentence1_result = create_token_dict(fill_mask(sentence1))\n",
    "sentence2_result = create_token_dict(fill_mask(sentence2))\n",
    "\n",
    "intersection = set(sentence1_result.keys()).intersection(set(sentence2_result.keys()))\n",
    "\n",
    "for token in intersection:\n",
    "    print(f\"{sentence1_result[token]['token_sequence']}({sentence1_result[token]['token_str']}:{sentence1_result[token]['token_score']}) --- {sentence2_result[token]['token_sequence']}({sentence2_result[token]['token_str']}:{sentence2_result[token]['token_score']})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T00:16:34.230865Z",
     "start_time": "2024-06-03T00:16:34.060508Z"
    }
   },
   "id": "8e266d245c4f4cbe",
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
